<style>

.reveal .slides > sectionx {
    top: -70%;
}

.reveal pre code.r {background-color: #ccF}
.reveal pre code {font-size: 1.3em}

.small-code pre code {
  font-size: 1.15em;
}

.section .reveal li {color:white}
.section .reveal em {font-weight: bold; font-style: "none"}

</style>



Corpus Analysis and Visualization
========================================================
author: Wouter van Atteveldt
date:   VU-HPC, 2017-11-05

```{r, echo=FALSE}
library(printr)
```

Course Overview
========================================================

10:30 - 12:00
- Recap: Frequency Based Analysis and the DTM
- Dictionary Analysis with AmCAT and R

13:30 - 15:00
- *Simple Natural Language Processing*
- Corpus Analysis and Visualization
- Topic Modeling and Visualization

15:15 - 17:00
- Sentiment Analysis with dictionaries
- Sentiment Analysis with proximity


Simple NLP
====

+ Preprocess documents to get more information
+ Relatively fast and accurate
  + Lemmatizing
  + Part-of-Speech (POS) tagging
  + Named Entity Recognition
+ Unfortunately, not within R

NLPipe + nlpiper
===

+ nlpipe: simple NLP processing based on stanford corenlp, others

```{sh, eval=F}
docker run --name corenlp -dp 9000:9000 chilland/corenlp-docker

docker run --name nlpipe --link corenlp:corenlp -e "CORENLP_HOST=http://corenlp:9000" -dp 5001:5001 vanatteveldt/nlpipe
```

```{r, eval=F}
devtools::install_github("vanatteveldt/nlpiper")
```
```{r}
library(nlpiper)
process("test_upper", "test")
```

Corenlp POS+lemma+NER
====

```{r, eval=FALSE}
library(nlpiper)
text = "Donald trump was elected president of the United States"
process("corenlp_lemmatize", text, format="csv")
```

NLPiper and US elections
===
class: small-code

+ You can lemmatize a set of articles or AmCAT set directly
+ But that can take a while..
+ Download tokens for US elections:

```{r, eval=FALSE}
# choose one:
download.file("http://i.amcat.nl/tokens.rds", "tokens.rds")
download.file("http://i.amcat.nl/tokens_full.rds", "tokens.rds")
download.file("http://i.amcat.nl/tokens_sample.rds", "tokens.rds")
```
```{r}
meta = readRDS("meta.rds")
tokens = readRDS("tokens.rds")
head(tokens)
```

Corpus Analysis
=====
type:section

Corpus Analysis
===

- Exploratory Analysis
- Term statistics
- Corpus comparison

The corpustools package
- Useful functions for corpus analysis
- Works on token list rather than dfm/dtm
  - preserves word order
  
```{r, eval=FALSE}  
install.packages("corpustools")
```

Create TCorpus from tokens
===

```{r}
library(corpustools)
tc = tokens_to_tcorpus(tokens, "id", sent_i_col = "sentence", )
tc_nouns = tc$subset(POS1=="N", copy = T)

dfm = tc$dtm('lemma', form = 'quanteda_dfm')
```

Corpus Comparison
===

```{r}
pre = subset(meta, medium == "The New York Times" & date < "2016-08-01")
post = subset(meta, medium == "The New York Times" & date >= "2016-08-01")

tc1 = tokens_to_tcorpus(subset(tokens, id %in% pre$id & POS1 == "G"), "id", sent_i_col = "sentence")
tc2 = tokens_to_tcorpus(subset(tokens, id %in% post$id & POS1 == "G"), "id", sent_i_col = "sentence")

cmp = tc1$compare_corpus(tc2, feature = 'lemma')
cmp = plyr::arrange(cmp, -chi2)
head(cmp)
```

Visualization
===
type:section

Visualization
===

```{r, fig.width=10, fig.height=10}
library(quanteda)
dfm = tc2$dtm('lemma', form='quanteda_dfm')
textplot_wordcloud(dfm, max.words = 50, scale = c(4, 0.5))
```

Beyond (stupid) word clouds
===

+ Word clouds waste most information
+ `corpustools::plotWords`
  + specify x, y, colour, size, etc.
+ Use any analytics you have to determine characteristics
+ See also http://vanatteveldt.com/lse-text-visualization/

Visualizing comparisons
===

```{r, fig.width=20, fig.height=8}
library(scales)
h = rescale(log(cmp$ratio), c(1, .6666))
s = rescale(sqrt(cmp$chi2), c(.25,1))
cmp$col = hsv(h, s, .33 + .67*s)
with(head(cmp, 75), plot_words(x=log(ratio), words=feature, wordfreq=chi2, random.y = T, col=col, scale=1))
```

Visualizing over time
===

```{r, fig.width=20, fig.height=8}
wordfreqs = tidytext::tidy(dfm)
wordfreqs = merge(meta, wordfreqs, by.x="id", by.y="document")
dates = aggregate(wordfreqs["date"], by=wordfreqs["term"], FUN=mean)
freqs = as.data.frame(table(wordfreqs$term))
terms = merge(dates, freqs, by.x="term", by.y="Var1")
terms = plyr::arrange(terms, -Freq)
with(head(terms, 50), plot_words(words=term, x=date, wordfreq = Freq))
axis(1)
```

Topic Modeling
===
type:section

Topic Models
===


Topic Models
===

```{r}
dtm = tc2$dtm(feature='lemma') # from tcorpus
# or: dtm = convert(dfm, to="topicmodels") # from quanteda

set.seed(1234)
library(topicmodels)
m = LDA(dtm, k = 10, method = "Gibbs", control = list(iter = 100, alpha=.1))
head(terms(m, 10))
```

===
Visualizing Topic Models: LDAvis

```{r, eval=F}
library(LDAvis)
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi <- posterior(m)$terms %>% as.matrix
theta <- posterior(m)$topics %>% as.matrix
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
json =  createJSON(phi = phi, theta = theta,
             vocab = vocab,
             doc.length = doc.length,
             term.frequency = term.freq)
LDAvis::serVis(json)
```



Visualizing Topic Models: heat map
=== 

```{r}
topics = c("books", "police", "culture", "city", "campaign", "movies", "email", "people", "songs", "economy" )
cm = cor(t(m@beta))
colnames(cm) = rownames(cm) = topics
diag(cm) = 0
heatmap(cm, symm = T)
```

Visualizing Topic Models: word clouds
=== 

```{r}
compare.topics <- function(m, cmp_topics) {
  docs = factor(m@wordassignments$i, labels=m@documents)
  terms = factor(m@wordassignments$j, labels=m@terms)
  assignments = data.frame(doc=docs, term=terms, freq=m@wordassignments$v)
  terms = dcast(assignments, term ~ freq, value.var = "doc", fun.aggregate = length)
  terms = terms[, c(1, cmp_topics+1)]
  terms$freq = rowSums(terms[-1])
  terms = terms[terms$freq > 0,]
  terms$prop = terms[[2]] / terms$freq
  terms$col = hsv(rescale(terms$prop, c(1, .6666)), .5, .5)
  terms[order(-terms$freq), ]
}
```

Visualizing Topic Models: word clouds
=== 
```{r, fig.width=20, fig.height=8}
terms = compare.topics(m, match(c("campaign", "email"), topics))
with(head(terms, 100), plot_words(x=prop, wordfreq = freq, words = term, col=col, xaxt="none", random.y = T, scale=2))
```


Analysing Topic Models
===
```{r}
tpd = posterior(m)$topics
colnames(tpd) = topics
tpd = merge(meta, tpd, by.x="id", by.y="row.names")
head(tpd)
```

Hands-on session II
===
type:section

- Corpus analysis of Election campaign 
  - (or your own data...)
- Which words, adjectives, verbs, etc are frequent?
- How do they differ over time, by medium, subcorpus
- What topics can we find?
- Can we visualize topics, contrasts, etc.
  
  